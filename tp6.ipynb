{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# warnings imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mouaz\\Desktop\\hamza\\Tp_TAL\\tp6.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp6.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m categories \u001b[39m=\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp6.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39malt.atheism\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp6.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39mtalk.religion.misc\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp6.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39mcomp.graphics\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp6.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39msci.space\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp6.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp6.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#import dataset\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp6.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m dataset \u001b[39m=\u001b[39m fetch_20newsgroups(subset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mall\u001b[39;49m\u001b[39m'\u001b[39;49m, categories\u001b[39m=\u001b[39;49mcategories, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp6.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#save labels\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp6.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m labels \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mtarget\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:284\u001b[0m, in \u001b[0;36mfetch_20newsgroups\u001b[1;34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[39mif\u001b[39;00m download_if_missing:\n\u001b[0;32m    283\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mDownloading 20news dataset. This may take a few minutes.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 284\u001b[0m     cache \u001b[39m=\u001b[39m _download_20newsgroups(\n\u001b[0;32m    285\u001b[0m         target_dir\u001b[39m=\u001b[39;49mtwenty_home, cache_path\u001b[39m=\u001b[39;49mcache_path\n\u001b[0;32m    286\u001b[0m     )\n\u001b[0;32m    287\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m20Newsgroups dataset not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:76\u001b[0m, in \u001b[0;36m_download_20newsgroups\u001b[1;34m(target_dir, cache_path)\u001b[0m\n\u001b[0;32m     73\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(target_dir)\n\u001b[0;32m     75\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mDownloading dataset from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (14 MB)\u001b[39m\u001b[39m\"\u001b[39m, ARCHIVE\u001b[39m.\u001b[39murl)\n\u001b[1;32m---> 76\u001b[0m archive_path \u001b[39m=\u001b[39m _fetch_remote(ARCHIVE, dirname\u001b[39m=\u001b[39;49mtarget_dir)\n\u001b[0;32m     78\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mDecompressing \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, archive_path)\n\u001b[0;32m     79\u001b[0m tarfile\u001b[39m.\u001b[39mopen(archive_path, \u001b[39m\"\u001b[39m\u001b[39mr:gz\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mextractall(path\u001b[39m=\u001b[39mtarget_dir)\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\datasets\\_base.py:1391\u001b[0m, in \u001b[0;36m_fetch_remote\u001b[1;34m(remote, dirname)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Helper function to download a remote dataset into path\u001b[39;00m\n\u001b[0;32m   1370\u001b[0m \n\u001b[0;32m   1371\u001b[0m \u001b[39mFetch a dataset pointed by remote's url, save into path using remote's\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[39m    Full path of the created file.\u001b[39;00m\n\u001b[0;32m   1388\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m file_path \u001b[39m=\u001b[39m remote\u001b[39m.\u001b[39mfilename \u001b[39mif\u001b[39;00m dirname \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m join(dirname, remote\u001b[39m.\u001b[39mfilename)\n\u001b[1;32m-> 1391\u001b[0m urlretrieve(remote\u001b[39m.\u001b[39;49murl, file_path)\n\u001b[0;32m   1392\u001b[0m checksum \u001b[39m=\u001b[39m _sha256(file_path)\n\u001b[0;32m   1393\u001b[0m \u001b[39mif\u001b[39;00m remote\u001b[39m.\u001b[39mchecksum \u001b[39m!=\u001b[39m checksum:\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:270\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    267\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[0;32m    269\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     block \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(bs)\n\u001b[0;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    272\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1312\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "'alt.atheism',\n",
    "'talk.religion.misc',\n",
    "'comp.graphics',\n",
    "'sci.space',\n",
    "]\n",
    "\n",
    "\n",
    "#import dataset\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
    "#save labels\n",
    "labels = dataset.target\n",
    "#get the unique labels\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(f\" documents{len(dataset.data)}\")\n",
    "print(f\" categories {len(dataset.target_names)}\")\n",
    "data = dataset.data\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english',\n",
    "use_idf=True)\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "\n",
    "#Dimensionality Reduction\n",
    "# Vectorizer results are normalized, which makes KMeans behave better\n",
    "    # Since LSA/SVD results are not normalized, we have to redo the normalization.\n",
    "\n",
    "    #If we do not normalize the data, variables with different scaling \n",
    "    # will be weighted differently in the distance formula \n",
    "    # that is being optimized during training.\n",
    "n_components = 5\n",
    "svd = TruncatedSVD(n_components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "#The final X is the input which we will be using. \n",
    "# It has been cleaned, TF-IDF transformed, and its dimensions reduced.\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "#scikit-learn offers two implementations of kmeans:\n",
    "# either in mini-batches or without\n",
    "minibatch = True\n",
    "if minibatch:\n",
    "   km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "   init_size=1000, batch_size=1000)\n",
    "else:\n",
    "   km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "km.fit(X)\n",
    "# top words per cluster\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(true_k):\n",
    "   print(\"Cluster %d:\" % i)\n",
    "   for ind in order_centroids[i, :10]:\n",
    "      print(' %s' % terms[ind])\n",
    "print(\"First method:\")\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f \"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "#Note: You might see different results, as machine learning \n",
    "# algorithms do not produce the exact same results each time.\n",
    "#km.predict(X_test) to test our model\n",
    "\n",
    "#imports the KMeans algorithm from the scikit-learn library and \n",
    "# creates an instance of it with three clusters, a random state of 0, \n",
    "# and automatic initialization\n",
    "#KMeans algorithm is a clustering algorithm that groups \n",
    "# similar data points together based on their distance from each other\n",
    "kmeans = KMeans(n_clusters = 3, random_state = 0, n_init='auto')\n",
    "#The fit method is then called on the normalized training data \n",
    "# to train the KMeans model on the data.\n",
    "kmeans.fit(X)\n",
    "print(\"Second method:\")\n",
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(true_k):\n",
    "   print(\"Cluster %d:\" % i)\n",
    "   for ind in order_centroids[i, :10]:\n",
    "      print(' %s' % terms[ind])\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f \"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#dataset = fetch_20newsgroups(download_if_missing=True)\n",
    "#news_list = dataset.data\n",
    "#news_list = news_list[-1000:]\n",
    "#print(news_list[1])\n",
    "\n",
    "#df = pd.DataFrame(np.array(news_list).reshape(-1,1), columns=['text'])\n",
    "\n",
    "#df[\"tokens\"]=df[\"text\"].apply(\n",
    "#     lambda x: [ \n",
    " #    preprocess_token(token)\n",
    "  #   for token in nlp(x)\n",
    "   #  if is_token_allowed(token)])\n",
    "\n",
    "#print(df[\"tokens\"].head)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
