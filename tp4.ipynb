{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from pathlib import Path\n",
    "import random \n",
    "import plac\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp = spacy.load(\"en_core_web_sm\")\\nsent_0 = nlp(u\\'Myriam saw Clement with a telescope.\\')\\nsent_1 = nlp(u\\'Self-driving cars shift insurance liability toward manufacturers.\\')\\nsent_2 = nlp(u\\'I shot the elephant in my pyjamas.\\')\\nfor chunk in sent_0.noun_chunks:\\n    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)\\nfor token in sent_0:\\n    print(\"--\",token.text, token.dep_, token.head.text, token.head.pos_,\\n    [child for child in token.children])'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''nlp = spacy.load(\"en_core_web_sm\")\n",
    "sent_0 = nlp(u'Myriam saw Clement with a telescope.')\n",
    "sent_1 = nlp(u'Self-driving cars shift insurance liability toward manufacturers.')\n",
    "sent_2 = nlp(u'I shot the elephant in my pyjamas.')\n",
    "for chunk in sent_0.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)\n",
    "for token in sent_0:\n",
    "    print(\"--\",token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "    [child for child in token.children])'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mouaz\\Desktop\\hamza\\Tp_TAL\\tp4.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp4.ipynb#W2sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m             doc \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39mmake_doc(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp4.ipynb#W2sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m             exampl \u001b[39m=\u001b[39m Example\u001b[39m.\u001b[39mfrom_dict(doc, annotations)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp4.ipynb#W2sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m             nlp\u001b[39m.\u001b[39;49mupdate([exampl], sgd\u001b[39m=\u001b[39;49moptimizer1,losses\u001b[39m=\u001b[39;49mlosses)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp4.ipynb#W2sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m             \u001b[39m#nlp.update([text], [annotations], sgd=optimizer1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp4.ipynb#W2sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         \u001b[39m#print(losses)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp4.ipynb#W2sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp4.ipynb#W2sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp4.ipynb#W2sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# test the trained model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mouaz/Desktop/hamza/Tp_TAL/tp4.ipynb#W2sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m test_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFew people attended the meeting.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\language.py:1193\u001b[0m, in \u001b[0;36mLanguage.update\u001b[1;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39mfor\u001b[39;00m name, proc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline:\n\u001b[0;32m   1191\u001b[0m     \u001b[39m# ignore statements are used here because mypy ignores hasattr\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mupdate\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1193\u001b[0m         proc\u001b[39m.\u001b[39;49mupdate(examples, sgd\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, losses\u001b[39m=\u001b[39;49mlosses, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg[name])  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m     \u001b[39mif\u001b[39;00m sgd \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1195\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1196\u001b[0m             name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude\n\u001b[0;32m   1197\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(proc, ty\u001b[39m.\u001b[39mTrainableComponent)\n\u001b[0;32m   1198\u001b[0m             \u001b[39mand\u001b[39;00m proc\u001b[39m.\u001b[39mis_trainable\n\u001b[0;32m   1199\u001b[0m             \u001b[39mand\u001b[39;00m proc\u001b[39m.\u001b[39mmodel \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1200\u001b[0m         ):\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:411\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.update\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:671\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser._init_gold_batch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\_parser_internals\\arc_eager.pyx:659\u001b[0m, in \u001b[0;36mspacy.pipeline._parser_internals.arc_eager.ArcEager.init_gold\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mouaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\_parser_internals\\arc_eager.pyx:677\u001b[0m, in \u001b[0;36mspacy.pipeline._parser_internals.arc_eager.ArcEager._replace_unseen_labels\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Load a blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "# Add a dependency parser to the pipeline\n",
    "# add the parser to the pipeline if it doesn't exist\n",
    "if 'parser' not in nlp.pipe_names:\n",
    "    parser = nlp.create_pipe('parser')\n",
    "    nlp.add_pipe('parser', first=True)\n",
    " # otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    parser = nlp.get_pipe('parser')\n",
    "\n",
    "tags=[\"amod\"]\n",
    "# Define training data with examples of the new \"Quantity\" dependency\n",
    "TRAINING_DATA = [\n",
    "    (\"Few people attended the meeting.\", {'heads': [1, 1, 1, 3,3,1], 'deps': ['amod', 'nsubj', 'ROOT', 'det', 'osubj','punct']}),\n",
    "    (\"Many students passed the exam.\", {'heads': [1, 1, 1, 3,3,1], 'deps': ['amod', 'nsubj', 'ROOT', 'det', 'osubj','punct']}),\n",
    "    #(\"There were enough chairs for everyone.\", {\"heads\": [2, 2, 4, 4, 5, 5], \"deps\": [\"expl\", \"ROOT\", \"amod\", \"nsubj\", \"prep\", \"pobj\"]}),\n",
    "    (\"Some students completed all the assignments.\", {'heads': [1, 1, 1, 3, 3, 3,1], 'deps': ['amod', 'nsubj', 'ROOT', 'det', 'det', 'osubj', 'punct']}),\n",
    "    #(\"Half of the pizza is gone.\", {\"heads\": [1, 1, 1, 5], \"deps\": [\"amod\", \"prep\", \"pobj\", \"ROOT\"]}),\n",
    "    #(\"The whole team worked together.\", {\"heads\": [1, 3, 3, 3], \"deps\": [\"det\", \"amod\", \"ROOT\", \"advmod\"]}),\n",
    "    #(\"Numerous books filled the shelves.\", {\"heads\": [1, 1, 3, 3, 3], \"deps\": [\"amod\", \"nsubj\", \"ROOT\", \"det\", \"dobj\"]}),\n",
    "    #(\"They trade mortgage-backed securities.\", {'heads': [1, 1, 4, 4, 5, 1, 1],'deps': ['nsubj', 'ROOT', 'compound', 'punct', 'nmod', 'dobj','punct']}),\n",
    "\n",
    "    # Add more examples as needed\n",
    "]\n",
    "\n",
    "# Train the dependency parser\n",
    "for _, annotations in TRAINING_DATA:\n",
    "    for dep in annotations.get('deps', []):\n",
    "        parser.add_label(dep)\n",
    "# get names of other pipes to disable them during training\n",
    "other_pipess = [pipe for pipe in nlp.pipe_names if pipe != 'parser']\n",
    "with nlp.disable_pipes(*other_pipess): # only train parser\n",
    "    optimizer1 = nlp.create_optimizer()\n",
    "    for itn in range(20):\n",
    "        random.shuffle(TRAINING_DATA)\n",
    "        losses = {}\n",
    "        for text, annotations in TRAINING_DATA:\n",
    "            doc = nlp.make_doc(text)\n",
    "            exampl = Example.from_dict(doc, annotations)\n",
    "            nlp.update([exampl], sgd=optimizer1,losses=losses)\n",
    "            #nlp.update([text], [annotations], sgd=optimizer1)\n",
    "        #print(losses)\n",
    "\n",
    "\n",
    "# test the trained model\n",
    "test_text = \"Few people attended the meeting.\"\n",
    "doc = nlp(test_text)\n",
    "print('Dependencies', [(t.text, t.dep_, t.head.text) for t in doc])\n",
    "# save model to output directory\n",
    "output_dir = Path(\"./models\")\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n",
    "    # test the saved model\n",
    "    print(\"Loading from\", output_dir)\n",
    "    nlp22 = spacy.load(output_dir)\n",
    "    doc = nlp22(test_text)\n",
    "    print('Dependencies', [(t.text, t.dep_, t.head.text) for t in doc])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
